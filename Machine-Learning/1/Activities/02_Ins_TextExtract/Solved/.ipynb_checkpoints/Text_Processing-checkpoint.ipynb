{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Python NaturalLanguageToolKit (NLTK) \n",
    "\n",
    "is a set of modules and corpora enabling the reader to do natural langauge processing against corpora of one or more texts. It goes beyond text minnig and provides tools to do machine learning, but this Notebook barely scratches that surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install NLTK & Download data\n",
    "\n",
    "* pip install nltk -- install nltk in the correct virtual environment\n",
    "* import nltk\n",
    "* nltk.download() -- pops up a GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Sententences Into Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE',\n",
       " '1',\n",
       " ':',\n",
       " '[',\n",
       " 'wind',\n",
       " ']',\n",
       " '[',\n",
       " 'clop',\n",
       " 'clops',\n",
       " 'clop',\n",
       " ']',\n",
       " 'KING',\n",
       " 'ARTHUR',\n",
       " ':',\n",
       " 'Whoa',\n",
       " 'there',\n",
       " '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# help(nltk.word_tokenize)\n",
    "my_sent = 'SCENE 1 : [wind] [ clop clops clop ] KING ARTHUR : Whoa there !'\n",
    "my_tokens = nltk.word_tokenize(my_sent)\n",
    "my_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bassic Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = my_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE', 'wind', 'clop', 'clops', 'clop', 'KING', 'ARTHUR', 'Whoa', 'there']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w for w in my_tokens if w.isalpha()]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of (English) stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words( 'english' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE', 'wind', 'clop', 'clops', 'clop', 'KING', 'ARTHUR', 'Whoa']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [w for w in words if w not in stopwords]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scene', 'wind', 'clop', 'clop', 'clop', 'king', 'arthur', 'whoa']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer     = PorterStemmer()\n",
    "stems       = [ stemmer.stem( w ) for w in words ]\n",
    "stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words( 'english' )\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words:  ['Fellow', 'Citizens', 'Senate', 'House', 'Representatives']\n",
      "Stems:  ['fellow', 'citizen', 'senat', 'hous', 'repres']\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    my_tokens = nltk.word_tokenize(text)\n",
    "    words = [w for w in my_tokens if w.isalpha()]\n",
    "    words = [w for w in words if w not in stopwords]\n",
    "    stems = [ stemmer.stem( w ) for w in words ]\n",
    "    return [words, stems]\n",
    "words, stems = process_text('Fellow - Citizens of the Senate and of the House of Representatives :')\n",
    "# words, stems = process_text('Fellow fellow - Citizens of the Senate and of the House of Representatives :')\n",
    "print('Words: ', words)\n",
    "print('Stems: ', stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
