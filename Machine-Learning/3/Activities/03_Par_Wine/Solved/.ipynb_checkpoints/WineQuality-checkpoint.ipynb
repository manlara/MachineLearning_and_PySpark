{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Data for this analysis comes from\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "import sklearn.metrics as sk_metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from imblearn.datasets import fetch_datasets\n",
    "from imblearn.pipeline import make_pipeline as make_pipeline_imb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_scores(headline, true_value, pred):\n",
    "    print(headline)\n",
    "    print(f\"accuracy: {accuracy_score(true_value, pred)}\")\n",
    "    print(f\"precision: {precision_score(true_value, pred)}\")\n",
    "    print(f\"recall: {recall_score(true_value, pred)}\")\n",
    "    print(f\"f1: {f1_score(true_value, pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model\n",
    "classifier = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "data = fetch_datasets()['wine_quality']\n",
    "print(f\"{data.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wine_quality'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({-1: 4715, 1: 183})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(data['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.0000e+00, 2.1000e-01, 3.8000e-01, 8.0000e-01, 2.0000e-02,\n",
       "       2.2000e+01, 9.8000e+01, 9.8941e-01, 3.2600e+00, 3.2000e-01,\n",
       "       1.1800e+01])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# columns = chemical properties of wines grown in the same region in Italy but derived from three different cultivars\n",
    "data['data'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['data'], data['target'], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(classifier(random_state=42))\n",
    "model = pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balanced Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE oversampling\n",
    "smote_pipeline = make_pipeline_imb(SMOTE(random_state=42), classifier(random_state=42))\n",
    "smote_model = smote_pipeline.fit(X_train, y_train)\n",
    "smote_prediction = smote_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NearMiss undersampling\n",
    "nearmiss_pipeline = make_pipeline_imb(NearMiss(random_state=42), classifier(random_state=42))\n",
    "nearmiss_model = nearmiss_pipeline.fit(X_train, y_train)\n",
    "nearmiss_prediction = nearmiss_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Distributions\n",
    "After applying SMOTE and NearMiss how many events are left compared to doing no balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal: Counter({-1: 4715, 1: 183})\n",
      "SMOTE: Counter({-1: 4715, 1: 4715})\n",
      "NearMiss: Counter({-1: 183, 1: 183})\n"
     ]
    }
   ],
   "source": [
    "print(f\"normal: {Counter(data['target'])}\")\n",
    "X_smote, y_smote = SMOTE().fit_sample(data['data'], data['target'])\n",
    "print(f\"SMOTE: {Counter(y_smote)}\")\n",
    "X_nearmiss, y_nearmiss = NearMiss().fit_sample(data['data'], data['target'])\n",
    "print(f\"NearMiss: {Counter(y_nearmiss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1225"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nearmiss_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      1.00      0.99      1186\n",
      "           1       0.67      0.21      0.31        39\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1225\n",
      "   macro avg       0.82      0.60      0.65      1225\n",
      "weighted avg       0.96      0.97      0.96      1225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(\"Normal Classification Report\")\n",
    "print(classification_report(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE Classification Report\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "         -1       0.98      0.97      0.44      0.98      0.65      0.45      1186\n",
      "          1       0.33      0.44      0.97      0.38      0.65      0.40        39\n",
      "\n",
      "avg / total       0.96      0.95      0.45      0.96      0.65      0.44      1225\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"SMOTE Classification Report\")\n",
    "print(classification_report_imbalanced(y_test, smote_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the improvement in the Recall score for predicting good wines.\n",
    "\n",
    "The F1 score for predicting bad wines drops by 1% but the prediction on good wines increases by 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (normal pipeline): 97.14285714285714%\n",
      "Accuracy (SMOTE pipeline): 95.42857142857143%\n",
      "Accurcay (NearMiss pipeline): 34.69387755102041%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy (normal pipeline): {pipeline.score(X_test,y_test)*100}%\")\n",
    "print(f\"Accuracy (SMOTE pipeline): {smote_pipeline.score(X_test,y_test)*100}%\")\n",
    "print(f\"Accurcay (NearMiss pipeline): {nearmiss_pipeline.score(X_test,y_test)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold Cross Validation\n",
    "\n",
    "Splitting data just one way can lead to bad estimations on the quality of the model. One technique for splitting the data for better evaluating models is [K-Fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42)\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "auc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation with Over or Under Sampled datasets\n",
    "\n",
    "When doing over sampling we are synthetically generating data so we need to careful when do that with cross validation. The entire dataset is first split and for each iteration the over or under sampling technique is applied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Correct Method\n",
    "\n",
    "Below is the right way to use K-Fold cross validation with over/under sampling methods\n",
    "\n",
    "Split raw data and then over/under sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9458184581734601\n",
      "precision: 0.32365203424026956\n",
      "recall: 0.32849492461561425\n",
      "f1: 0.318705450879618\n"
     ]
    }
   ],
   "source": [
    "for train, test in kf.split(X_train, y_train):\n",
    "    pipeline = make_pipeline_imb(SMOTE(), classifier(random_state=42))\n",
    "    model = pipeline.fit(X_train[train], y_train[train])\n",
    "    prediction = model.predict(X_train[test])\n",
    "    \n",
    "    accuracy.append(pipeline.score(X_train[test], y_train[test]))\n",
    "    precision.append(sk_metrics.precision_score(y_train[test], prediction))\n",
    "    recall.append(sk_metrics.recall_score(y_train[test], prediction))\n",
    "    f1.append(sk_metrics.f1_score(y_train[test], prediction))\n",
    "    auc.append(sk_metrics.roc_auc_score(y_train[test], prediction))\n",
    "\n",
    "print(f\"accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"precision: {np.mean(precision)}\")\n",
    "print(f\"recall: {np.mean(recall)}\")\n",
    "print(f\"f1: {np.mean(f1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrong Way To Split Data with Over/Under Sampling\n",
    "Don't do this as it brings in synthetic data into your testing data (data leakage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9462710984940378\n",
      "precision: 0.5441232084418782\n",
      "recall: 0.6384699254281219\n",
      "f1: 0.5682206821921938\n"
     ]
    }
   ],
   "source": [
    "# GOTCHA\n",
    "X_bad, y_bad = SMOTE().fit_sample(X_train, y_train)\n",
    "for train, test in kf.split(X_bad, y_bad):\n",
    "    pipeline = make_pipeline(classifier(random_state=42))\n",
    "    model = pipeline.fit(X_bad[train], y_bad[train])\n",
    "    prediction = model.predict(X_bad[test])\n",
    "    \n",
    "    accuracy.append(pipeline.score(X_bad[test], y_bad[test]))\n",
    "    precision.append(sk_metrics.precision_score(y_bad[test], prediction))\n",
    "    recall.append(sk_metrics.recall_score(y_bad[test], prediction))\n",
    "    f1.append(sk_metrics.f1_score(y_bad[test], prediction))\n",
    "\n",
    "print(f\"accuracy: {np.mean(accuracy)}\")\n",
    "print(f\"precision: {np.mean(precision)}\")\n",
    "print(f\"recall: {np.mean(recall)}\")\n",
    "print(f\"f1: {np.mean(f1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData]",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
