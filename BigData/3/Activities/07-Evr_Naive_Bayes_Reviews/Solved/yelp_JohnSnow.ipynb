{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Additional Python Libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding environment variable `PYSPARK_SUBMIT_ARGS`\n",
      "--packages com.databricks:spark-csv_2.11:1.5.0,JohnSnowLabs:spark-nlp:2.3.4 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "from spark_libs import spark_submit\n",
    "packages = [\"com.databricks:spark-csv_2.11:1.5.0\", \n",
    "            \"JohnSnowLabs:spark-nlp:2.3.4\"]\n",
    "spark_submit(packages=packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml.feature import (\n",
    "    StringIndexer,\n",
    "    HashingTF, \n",
    "    IDF\n",
    ")\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "\n",
    "app_name = \"yelp-john-snow\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.4 MB\n",
      "[OK!]\n",
      "{'stem': ['we', 'ar', 'veri', 'happi', 'about', 'sparknlp'], 'checked': ['We', 'are', 'very', 'happy', 'about', 'SparkNLP'], 'lemma': ['We', 'be', 'very', 'happy', 'about', 'SparkNLP'], 'document': ['We are very happy about SparkNLP'], 'pos': ['PRP', 'VBP', 'RB', 'JJ', 'IN', 'NNP'], 'token': ['We', 'are', 'very', 'happy', 'about', 'SparkNLP'], 'sentence': ['We are very happy about SparkNLP']}\n"
     ]
    }
   ],
   "source": [
    "import sparknlp\n",
    "sparknlp.start()\n",
    "\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "explain_document_pipeline = PretrainedPipeline(\"explain_document_ml\")\n",
    "annotations = explain_document_pipeline.annotate(\"We are very happy about SparkNLP\")\n",
    "print(annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(annotations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- class: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url =\"https://s3.amazonaws.com/dataviz-curriculum/day_2/yelp_reviews.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "\n",
    "df = spark.read \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .options(header='true', inferSchema=\"true\") \\\n",
    "    .load(SparkFiles.get(\"yelp_reviews.csv\"))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(class='positive', text='Wow... Loved this place.')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show first row\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|   class|                text|            document|            sentence|               token|             checked|               lemma|                stem|                 pos|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|positive|Wow... Loved this...|[[document, 0, 23...|[[document, 0, 5,...|[[token, 0, 2, Wo...|[[token, 0, 2, Wo...|[[token, 0, 2, Wo...|[[token, 0, 2, wo...|[[pos, 0, 2, UH, ...|\n",
      "|negative|  Crust is not good.|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 4, Cr...|[[token, 0, 4, Cr...|[[token, 0, 4, Cr...|[[token, 0, 4, cr...|[[pos, 0, 4, NNP,...|\n",
      "|negative|Not tasty and the...|[[document, 0, 40...|[[document, 0, 40...|[[token, 0, 2, No...|[[token, 0, 2, No...|[[token, 0, 2, No...|[[token, 0, 2, no...|[[pos, 0, 2, RB, ...|\n",
      "|positive|Stopped by during...|[[document, 0, 86...|[[document, 0, 86...|[[token, 0, 6, St...|[[token, 0, 6, St...|[[token, 0, 6, St...|[[token, 0, 6, st...|[[pos, 0, 6, NNP,...|\n",
      "|positive|The selection on ...|[[document, 0, 58...|[[document, 0, 58...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|\n",
      "|negative|Now I am getting ...|[[document, 0, 45...|[[document, 0, 45...|[[token, 0, 2, No...|[[token, 0, 2, No...|[[token, 0, 2, No...|[[token, 0, 2, no...|[[pos, 0, 2, RB, ...|\n",
      "|negative|Honeslty it didn'...|[[document, 0, 36...|[[document, 0, 35...|[[token, 0, 7, Ho...|[[token, 0, 7, Ho...|[[token, 0, 7, Ho...|[[token, 0, 7, ho...|[[pos, 0, 7, NNP,...|\n",
      "|negative|The potatoes were...|[[document, 0, 11...|[[document, 0, 11...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|\n",
      "|positive|The fries were gr...|[[document, 0, 24...|[[document, 0, 24...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|\n",
      "|positive|      A great touch.|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 0, A,...|[[token, 0, 0, A,...|[[token, 0, 0, A,...|[[token, 0, 0, a,...|[[pos, 0, 0, DT, ...|\n",
      "|positive|Service was very ...|[[document, 0, 23...|[[document, 0, 23...|[[token, 0, 6, Se...|[[token, 0, 6, Se...|[[token, 0, 6, Se...|[[token, 0, 6, se...|[[pos, 0, 6, NNP,...|\n",
      "|negative|  Would not go back.|[[document, 0, 17...|[[document, 0, 17...|[[token, 0, 4, Wo...|[[token, 0, 4, Wo...|[[token, 0, 4, Wo...|[[token, 0, 4, wo...|[[pos, 0, 4, MD, ...|\n",
      "|negative|The cashier had n...|[[document, 0, 98...|[[document, 0, 98...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, Th...|[[token, 0, 2, th...|[[pos, 0, 2, DT, ...|\n",
      "|positive|I tried the Cape ...|[[document, 0, 58...|[[document, 0, 51...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|[[token, 0, 0, i,...|[[pos, 0, 0, PRP,...|\n",
      "|negative|I was disgusted b...|[[document, 0, 61...|[[document, 0, 61...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|[[token, 0, 0, i,...|[[pos, 0, 0, PRP,...|\n",
      "|negative|I was shocked bec...|[[document, 0, 49...|[[document, 0, 49...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|[[token, 0, 0, I,...|[[token, 0, 0, i,...|[[pos, 0, 0, PRP,...|\n",
      "|positive| Highly recommended.|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 5, Hi...|[[token, 0, 5, Hi...|[[token, 0, 5, Hi...|[[token, 0, 5, hi...|[[pos, 0, 5, RB, ...|\n",
      "|negative|Waitress was a li...|[[document, 0, 37...|[[document, 0, 37...|[[token, 0, 7, Wa...|[[token, 0, 7, wa...|[[token, 0, 7, wa...|[[token, 0, 7, wa...|[[pos, 0, 7, NN, ...|\n",
      "|negative|This place is not...|[[document, 0, 50...|[[document, 0, 50...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, Th...|[[token, 0, 3, th...|[[pos, 0, 3, DT, ...|\n",
      "|negative|did not like at all.|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, di...|[[token, 0, 2, di...|[[token, 0, 2, do...|[[token, 0, 2, di...|[[pos, 0, 2, VBD,...|\n",
      "+--------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotations_df = explain_document_pipeline.transform(df)\n",
    "annotations_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|   class|                stem|\n",
      "+--------+--------------------+\n",
      "|positive|[[token, 0, 2, wo...|\n",
      "|negative|[[token, 0, 4, cr...|\n",
      "|negative|[[token, 0, 2, no...|\n",
      "|positive|[[token, 0, 6, st...|\n",
      "|positive|[[token, 0, 2, th...|\n",
      "|negative|[[token, 0, 2, no...|\n",
      "|negative|[[token, 0, 7, ho...|\n",
      "|negative|[[token, 0, 2, th...|\n",
      "|positive|[[token, 0, 2, th...|\n",
      "|positive|[[token, 0, 0, a,...|\n",
      "|positive|[[token, 0, 6, se...|\n",
      "|negative|[[token, 0, 4, wo...|\n",
      "|negative|[[token, 0, 2, th...|\n",
      "|positive|[[token, 0, 0, i,...|\n",
      "|negative|[[token, 0, 0, i,...|\n",
      "|negative|[[token, 0, 0, i,...|\n",
      "|positive|[[token, 0, 5, hi...|\n",
      "|negative|[[token, 0, 7, wa...|\n",
      "|negative|[[token, 0, 3, th...|\n",
      "|negative|[[token, 0, 2, di...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "annotations_df.select(['class', 'stem']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DocumentAssembler: Getting data in\n",
    "In order to get through the NLP process, we need to get raw data annotated. There is a special transformer that does this for us: the DocumentAssembler, it creates the first annotation of type Document which may be used by annotators down the road\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Building our own pipeline\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence detection and tokenization\n",
    "In this quick example, we now proceed to identify the sentences in each of our document lines. SentenceDetector requires a Document annotation, which is provided by the DocumentAssembler output, and it’s itself a Document type token. The Tokenizer requires a Document annotation type, meaning it works both with DocumentAssembler or SentenceDetector output, in here, we use the sentence output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDetector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finisher: Getting data out\n",
    "\n",
    "At the end of each pipeline or any stage that was done by Spark NLP, you may want to get results out whether onto another pipeline or simply write them on disk. The Finisher annotator helps you to clean the metadata (if it’s set to true) and output the results into an array:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setIncludeMetadata(True) \\\n",
    "    .setCleanAnnotations(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Spark ML Pipeline\n",
    "Now we want to put all this together and retrieve the results, we use a Pipeline for this. We use the same data in fit() that we will use in transform since none of the pipeline stages have a training stage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    finisher\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-----------------------+\n",
      "|   class|                text|      finished_token|finished_token_metadata|\n",
      "+--------+--------------------+--------------------+-----------------------+\n",
      "|positive|Wow... Loved this...|[Wow, ..., Loved,...|   [[sentence, 0], [...|\n",
      "|negative|  Crust is not good.|[Crust, is, not, ...|   [[sentence, 0], [...|\n",
      "|negative|Not tasty and the...|[Not, tasty, and,...|   [[sentence, 0], [...|\n",
      "|positive|Stopped by during...|[Stopped, by, dur...|   [[sentence, 0], [...|\n",
      "|positive|The selection on ...|[The, selection, ...|   [[sentence, 0], [...|\n",
      "|negative|Now I am getting ...|[Now, I, am, gett...|   [[sentence, 0], [...|\n",
      "|negative|Honeslty it didn'...|[Honeslty, it, di...|   [[sentence, 0], [...|\n",
      "|negative|The potatoes were...|[The, potatoes, w...|   [[sentence, 0], [...|\n",
      "|positive|The fries were gr...|[The, fries, were...|   [[sentence, 0], [...|\n",
      "|positive|      A great touch.|[A, great, touch, .]|   [[sentence, 0], [...|\n",
      "|positive|Service was very ...|[Service, was, ve...|   [[sentence, 0], [...|\n",
      "|negative|  Would not go back.|[Would, not, go, ...|   [[sentence, 0], [...|\n",
      "|negative|The cashier had n...|[The, cashier, ha...|   [[sentence, 0], [...|\n",
      "|positive|I tried the Cape ...|[I, tried, the, C...|   [[sentence, 0], [...|\n",
      "|negative|I was disgusted b...|[I, was, disguste...|   [[sentence, 0], [...|\n",
      "|negative|I was shocked bec...|[I, was, shocked,...|   [[sentence, 0], [...|\n",
      "|positive| Highly recommended.|[Highly, recommen...|   [[sentence, 0], [...|\n",
      "|negative|Waitress was a li...|[Waitress, was, a...|   [[sentence, 0], [...|\n",
      "|negative|This place is not...|[This, place, is,...|   [[sentence, 0], [...|\n",
      "|negative|did not like at all.|[did, not, like, ...|   [[sentence, 0], [...|\n",
      "+--------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted = model.transform(df)\n",
    "extracted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_neg_to_num = StringIndexer(inputCol='class',outputCol='label')\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"finished_token\", outputCol='hash_token')\n",
    "idf = IDF(inputCol='hash_token', outputCol='idf_token')\n",
    "# Create feature vectors\n",
    "features = VectorAssembler(inputCols=['idf_token'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    pos_neg_to_num,\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    finisher,\n",
    "    hashingTF,\n",
    "    idf,\n",
    "    features\n",
    "  ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+-----------------------+--------------------+--------------------+--------------------+\n",
      "|   class|                text|label|      finished_token|finished_token_metadata|          hash_token|           idf_token|            features|\n",
      "+--------+--------------------+-----+--------------------+-----------------------+--------------------+--------------------+--------------------+\n",
      "|positive|Wow... Loved this...|  0.0|[Wow, ..., Loved,...|   [[sentence, 0], [...|(262144,[1536,131...|(262144,[1536,131...|(262144,[1536,131...|\n",
      "|negative|  Crust is not good.|  1.0|[Crust, is, not, ...|   [[sentence, 0], [...|(262144,[1536,158...|(262144,[1536,158...|(262144,[1536,158...|\n",
      "|negative|Not tasty and the...|  1.0|[Not, tasty, and,...|   [[sentence, 0], [...|(262144,[1536,255...|(262144,[1536,255...|(262144,[1536,255...|\n",
      "|positive|Stopped by during...|  0.0|[Stopped, by, dur...|   [[sentence, 0], [...|(262144,[1536,339...|(262144,[1536,339...|(262144,[1536,339...|\n",
      "|positive|The selection on ...|  0.0|[The, selection, ...|   [[sentence, 0], [...|(262144,[1536,255...|(262144,[1536,255...|(262144,[1536,255...|\n",
      "|negative|Now I am getting ...|  1.0|[Now, I, am, gett...|   [[sentence, 0], [...|(262144,[1536,246...|(262144,[1536,246...|(262144,[1536,246...|\n",
      "|negative|Honeslty it didn'...|  1.0|[Honeslty, it, di...|   [[sentence, 0], [...|(262144,[1536,120...|(262144,[1536,120...|(262144,[1536,120...|\n",
      "|negative|The potatoes were...|  1.0|[The, potatoes, w...|   [[sentence, 0], [...|(262144,[1536,963...|(262144,[1536,963...|(262144,[1536,963...|\n",
      "|positive|The fries were gr...|  0.0|[The, fries, were...|   [[sentence, 0], [...|(262144,[1536,138...|(262144,[1536,138...|(262144,[1536,138...|\n",
      "|positive|      A great touch.|  0.0|[A, great, touch, .]|   [[sentence, 0], [...|(262144,[1536,138...|(262144,[1536,138...|(262144,[1536,138...|\n",
      "|positive|Service was very ...|  0.0|[Service, was, ve...|   [[sentence, 0], [...|(262144,[1536,255...|(262144,[1536,255...|(262144,[1536,255...|\n",
      "|negative|  Would not go back.|  1.0|[Would, not, go, ...|   [[sentence, 0], [...|(262144,[1536,132...|(262144,[1536,132...|(262144,[1536,132...|\n",
      "|negative|The cashier had n...|  1.0|[The, cashier, ha...|   [[sentence, 0], [...|(262144,[1536,362...|(262144,[1536,362...|(262144,[1536,362...|\n",
      "|positive|I tried the Cape ...|  0.0|[I, tried, the, C...|   [[sentence, 0], [...|(262144,[1536,289...|(262144,[1536,289...|(262144,[1536,289...|\n",
      "|negative|I was disgusted b...|  1.0|[I, was, disguste...|   [[sentence, 0], [...|(262144,[1536,255...|(262144,[1536,255...|(262144,[1536,255...|\n",
      "|negative|I was shocked bec...|  1.0|[I, was, shocked,...|   [[sentence, 0], [...|(262144,[1536,255...|(262144,[1536,255...|(262144,[1536,255...|\n",
      "|positive| Highly recommended.|  0.0|[Highly, recommen...|   [[sentence, 0], [...|(262144,[1536,700...|(262144,[1536,700...|(262144,[1536,700...|\n",
      "|negative|Waitress was a li...|  1.0|[Waitress, was, a...|   [[sentence, 0], [...|(262144,[1536,241...|(262144,[1536,241...|(262144,[1536,241...|\n",
      "|negative|This place is not...|  1.0|[This, place, is,...|   [[sentence, 0], [...|(262144,[1536,158...|(262144,[1536,158...|(262144,[1536,158...|\n",
      "|negative|did not like at all.|  1.0|[did, not, like, ...|   [[sentence, 0], [...|(262144,[1536,135...|(262144,[1536,135...|(262144,[1536,135...|\n",
      "+--------+--------------------+-----+--------------------+-----------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(df)\n",
    "extracted = model.transform(df)\n",
    "extracted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break data down into a training set and a testing set\n",
    "training, testing = extracted.randomSplit([0.7, 0.3])\n",
    "\n",
    "# Create a Naive Bayes model and fit training data\n",
    "nb = NaiveBayes()\n",
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|   class|                text|label|      finished_token|finished_token_metadata|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
      "+--------+--------------------+-----+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|negative|\"I don't know wha...|  1.0|[\", I, don't, kno...|   [[sentence, 0], [...|(262144,[15889,29...|(262144,[15889,29...|(262144,[15889,29...|[-723.82082134981...|[7.14288495719912...|       1.0|\n",
      "|negative|\"The burger... I ...|  1.0|[\", The, burger, ...|   [[sentence, 0], [...|(262144,[1536,963...|(262144,[1536,963...|(262144,[1536,963...|[-763.76920546547...|[6.37156015969899...|       1.0|\n",
      "|negative|              #NAME?|  1.0|          [#NAME, ?]|   [[sentence, 0], [...|(262144,[17160,37...|(262144,[17160,37...|(262144,[17160,37...|[-108.17403651085...|[0.00107798314402...|       1.0|\n",
      "|negative|2 times - Very Ba...|  1.0|[2, times, -, Ver...|   [[sentence, 0], [...|(262144,[28990,45...|(262144,[28990,45...|(262144,[28990,45...|[-377.35451689375...|[0.00208400610278...|       1.0|\n",
      "|negative|AVOID THIS ESTABL...|  1.0|[AVOID, THIS, EST...|   [[sentence, 0], [...|(262144,[28990,70...|(262144,[28990,70...|(262144,[28990,70...|[-249.67996932516...|[0.77333121683362...|       0.0|\n",
      "+--------+--------------------+-----+--------------------+-----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tranform the model with the testing data\n",
    "test_results = predictor.transform(testing)\n",
    "test_results.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting reviews was: 0.736583\n"
     ]
    }
   ],
   "source": [
    "# Use the Class Evaluator for a cleaner description\n",
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.10.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
