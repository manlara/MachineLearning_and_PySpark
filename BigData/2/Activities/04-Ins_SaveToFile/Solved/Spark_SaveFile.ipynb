{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Additional Python Libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding environment variable `PYSPARK_SUBMIT_ARGS`\n",
      "--packages com.databricks:spark-csv_2.11:1.5.0,org.postgresql:postgresql:42.2.9,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# Need postgres\n",
    "# https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "from spark_libs import spark_submit\n",
    "packages = [\"com.databricks:spark-csv_2.11:1.5.0\", \n",
    "            \"org.postgresql:postgresql:42.2.9\",\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\"]\n",
    "spark_submit(packages=packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from spark_connections import postgresUrlProperties, createMongoURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "\n",
    "app_name = \"spark-mongo-postgres-save-file\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://host.docker.internal:27017/customers.customer_data\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- car: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_connection = {\n",
    "    \"hostname\": \"host.docker.internal\",\n",
    "    \"port\": \"27017\"\n",
    "}\n",
    "mongo_database = \"customers\"\n",
    "sample_size = 1000 # how many rows to use to determine the schema\n",
    "\n",
    "collection = \"customer_data\"\n",
    "mongoURI = createMongoURI(mongo_connection, mongo_database, collection)\n",
    "print(mongoURI)\n",
    "\n",
    "df_customer_data = spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\",mongoURI) \\\n",
    "    .option(\"sampleSize\", sample_size) \\\n",
    "    .load()\n",
    "df_customer_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save as a csv file all fields must be flattened\n",
    "df_customer_data_flat = df_customer_data.withColumn(\"_id\", F.col(\"_id.oid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mode = \"overwrite\" # options: error, append, overwrite\n",
    "\n",
    "# codec=\"gzip\"\n",
    "# header=\"true\"\n",
    "df_customer_data_flat.write \\\n",
    "    .format(\"com.databricks.spark.csv\") \\\n",
    "    .options(header=\"true\") \\\n",
    "    .mode(save_mode) \\\n",
    "    .save(\"customer_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_id,car,email,first_name,gender,id,last_name\r\n",
      "5df292055be4f706164a5324,Scion,bcancott0@studiopress.com,Benetta,Female,1,Cancott\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 customer_data.csv/*csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as JSON\n",
    "\n",
    "Complex column types are allowed to be saved in JSON format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mode = \"overwrite\" # options: error, append, overwrite\n",
    "\n",
    "# options:\n",
    "# compression=\"gzip\"\n",
    "\n",
    "df_customer_data.write \\\n",
    "    .format(\"json\") \\\n",
    "    .options() \\\n",
    "    .mode(save_mode) \\\n",
    "    .save(\"customer_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_id\":{\"oid\":\"5df292055be4f706164a5324\"},\"car\":\"Scion\",\"email\":\"bcancott0@studiopress.com\",\"first_name\":\"Benetta\",\"gender\":\"Female\",\"id\":1,\"last_name\":\"Cancott\"}\r\n",
      "{\"_id\":{\"oid\":\"5df292055be4f706164a5325\"},\"car\":\"Chrysler\",\"email\":\"lcherry1@deliciousdays.com\",\"first_name\":\"Lilyan\",\"gender\":\"Female\",\"id\":2,\"last_name\":\"Cherry\"}\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 2 customer_data.json/*json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data as Parquet\n",
    "\n",
    "Parquet, an open source file format for Hadoop. Parquet stores nested data structures in a flat columnar format. Compared to a traditional approach where data is stored in row-oriented approach, parquet is more efficient in terms of storage and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mode = \"overwrite\" # options: error, append, overwrite\n",
    "\n",
    "df_customer_data.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .options() \\\n",
    "    .mode(save_mode) \\\n",
    "    .save(\"customer_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- car: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"customer_data.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as ORC\n",
    "\n",
    "The Optimized Row Columnar (ORC) file format provides a highly efficient way to store Hive data. It was designed to overcome limitations of the other Hive file formats. Using ORC files improves performance when Hive is reading, writing, and processing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mode = \"overwrite\" # options: error, append, overwrite\n",
    "\n",
    "df_customer_data.write \\\n",
    "    .format(\"orc\") \\\n",
    "    .options() \\\n",
    "    .mode(save_mode) \\\n",
    "    .save(\"customer_data.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- car: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"orc\").load(\"customer_data.orc\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
