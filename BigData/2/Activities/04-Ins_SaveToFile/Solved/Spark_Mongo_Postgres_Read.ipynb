{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Additional Python Libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding environment variable `PYSPARK_SUBMIT_ARGS`\n",
      "--packages com.databricks:spark-csv_2.11:1.5.0,org.postgresql:postgresql:42.2.9,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# Need postgres\n",
    "# https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "from spark_libs import spark_submit\n",
    "packages = [\"com.databricks:spark-csv_2.11:1.5.0\", \n",
    "            \"org.postgresql:postgresql:42.2.9\",\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\"]\n",
    "spark_submit(packages=packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from spark_connections import postgresUrlProperties, createMongoURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "\n",
    "app_name = \"spark-mongo-postgres-read\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://host.docker.internal:27017/customers.customer_data\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- car: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_connection = {\n",
    "    \"hostname\": \"host.docker.internal\",\n",
    "    \"port\": \"27017\"\n",
    "}\n",
    "mongo_database = \"customers\"\n",
    "sample_size = 1000 # how many rows to use to determine the schema\n",
    "\n",
    "collection = \"customer_data\"\n",
    "mongoURI = createMongoURI(mongo_connection, mongo_database, collection)\n",
    "print(mongoURI)\n",
    "\n",
    "df_customer_data = spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\",mongoURI) \\\n",
    "    .option(\"sampleSize\", sample_size) \\\n",
    "    .load()\n",
    "df_customer_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jdbc:postgresql://host.docker.internal:5433/customers \n",
      " {'user': 'postgres', 'password': 'changeme', 'driver': 'org.postgresql.Driver'}\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- us_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "postgres_connection = {\n",
    "    \"host\": \"host.docker.internal\",\n",
    "    \"port\": \"5433\", # should be 5432 for you\n",
    "    \"database\": \"customers\",\n",
    "    \"dialect\": \"postgresql\",\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"changeme\"\n",
    "}\n",
    "\n",
    "postgres_url, postgres_props = postgresUrlProperties(postgres_connection)\n",
    "print(postgres_url, \"\\n\", postgres_props)\n",
    "table = \"customer_location\"\n",
    "\n",
    "df_customer_location = spark.read.jdbc(url=postgres_url, table=table, properties=postgres_props)\n",
    "df_customer_location.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
