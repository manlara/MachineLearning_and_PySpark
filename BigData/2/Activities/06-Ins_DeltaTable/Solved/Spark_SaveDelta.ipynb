{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Additional Python Libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding environment variable `PYSPARK_SUBMIT_ARGS`\n",
      "--packages com.databricks:spark-csv_2.11:1.5.0,org.postgresql:postgresql:42.2.9,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,io.delta:delta-core_2.11:0.4.0 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# Need postgres\n",
    "# https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "from spark_libs import spark_submit\n",
    "packages = [\"com.databricks:spark-csv_2.11:1.5.0\", \n",
    "            \"org.postgresql:postgresql:42.2.9\",\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\",\n",
    "            \"io.delta:delta-core_2.11:0.4.0\"]\n",
    "spark_submit(packages=packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from spark_connections import postgresUrlProperties, createMongoURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "\n",
    "app_name = \"spark-mongo-postgres-save-delta\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://host.docker.internal:27017/users.user_data\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- active_user: boolean (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- street_address: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_connection = {\n",
    "    \"hostname\": \"host.docker.internal\",\n",
    "    \"port\": \"27017\"\n",
    "}\n",
    "mongo_database = \"users\"\n",
    "sample_size = 1000 # how many rows to use to determine the schema\n",
    "\n",
    "collection = \"user_data\"\n",
    "mongoURI = createMongoURI(mongo_connection, mongo_database, collection)\n",
    "print(mongoURI)\n",
    "\n",
    "df_user_data = spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\",mongoURI) \\\n",
    "    .option(\"sampleSize\", sample_size) \\\n",
    "    .load()\n",
    "df_user_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data as Delta Table\n",
    "\n",
    "Parquet, an open source file format for Hadoop. Parquet stores nested data structures in a flat columnar format. Compared to a traditional approach where data is stored in row-oriented approach, parquet is more efficient in terms of storage and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mode = \"overwrite\" # options: error, append, overwrite\n",
    "\n",
    "df_user_data.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .options() \\\n",
    "    .mode(save_mode) \\\n",
    "    .save(\"delta/user_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, \"delta/user_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.delete(\"id = 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|      1|2019-12-12 20:08:43|  null|    null|   DELETE|[predicate -> [\"(...|null|    null|     null|          0|          null|        false|\n",
      "|      0|2019-12-12 20:02:06|  null|    null|    WRITE|[mode -> Overwrit...|null|    null|     null|       null|          null|        false|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+---+---------+-------+-------------------+--------+\n",
      "|                 _id|active_user|first_name| id|last_name|  state|     street_address|username|\n",
      "+--------------------+-----------+----------+---+---------+-------+-------------------+--------+\n",
      "|[5df2872a9e748e56...|      false|       Caz|  2|   Felgat|Alabama|83 Hazelcrest Place|wwaller1|\n",
      "+--------------------+-----------+----------+---+---------+-------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().filter(\"id = 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable.update(\"id = 2\", {\"active_user\": F.lit(True)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+---+---------+-------+-------------------+--------+\n",
      "|                 _id|active_user|first_name| id|last_name|  state|     street_address|username|\n",
      "+--------------------+-----------+----------+---+---------+-------+-------------------+--------+\n",
      "|[5df2872a9e748e56...|       true|       Caz|  2|   Felgat|Alabama|83 Hazelcrest Place|wwaller1|\n",
      "+--------------------+-----------+----------+---+---------+-------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.toDF().filter(\"id = 2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "|      2|2019-12-12 20:14:48|  null|    null|   UPDATE|[predicate -> (id...|null|    null|     null|          1|          null|        false|\n",
      "|      1|2019-12-12 20:08:43|  null|    null|   DELETE|[predicate -> [\"(...|null|    null|     null|          0|          null|        false|\n",
      "|      0|2019-12-12 20:02:06|  null|    null|    WRITE|[mode -> Overwrit...|null|    null|     null|       null|          null|        false|\n",
      "+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method vacuum in module delta.tables:\n",
      "\n",
      "vacuum(retentionHours=None) method of delta.tables.DeltaTable instance\n",
      "    Recursively delete files and directories in the table that are not needed by the table for\n",
      "    maintaining older versions up to the given retention threshold. This method will return an\n",
      "    empty DataFrame on successful completion.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        deltaTable.vacuum()     # vacuum files not required by versions more than 7 days old\n",
      "    \n",
      "        deltaTable.vacuum(100)  # vacuum files not required by versions more than 100 hours old\n",
      "    \n",
      "    :param retentionHours: Optional number of hours retain history. If not specified, then the\n",
      "                           default retention period of 168 hours (7 days) will be used.\n",
      "    \n",
      "    .. note:: Evolving\n",
      "    \n",
      "    .. versionadded:: 0.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(deltaTable.vacuum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
