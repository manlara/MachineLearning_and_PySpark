{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Additional Python Libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding environment variable `PYSPARK_SUBMIT_ARGS`\n",
      "--packages com.databricks:spark-csv_2.11:1.5.0,org.postgresql:postgresql:42.2.9,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1,io.delta:delta-core_2.11:0.4.0 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# Need postgres\n",
    "# https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "from spark_libs import spark_submit\n",
    "packages = [\"com.databricks:spark-csv_2.11:1.5.0\", \n",
    "            \"org.postgresql:postgresql:42.2.9\",\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\",\n",
    "            \"io.delta:delta-core_2.11:0.4.0\"]\n",
    "spark_submit(packages=packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "\n",
    "app_name = \"spark-word-count\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from delta.tables import *\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous delta-tables\n",
    "\n",
    "files = [\"delta/delta-table1\", \"delta/delta-table2\"]\n",
    "for i in files:\n",
    "    try:\n",
    "        shutil.rmtree(i)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(returnType=StringType())\n",
    "def randomWordGenerator():\n",
    "    return random.choice([\"hello\", \"I\", \"am\", \"happy\", \"to\", \"run\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 8 rows\n",
    "\n",
    "data = spark.range(8)\n",
    "data = data.withColumn(\"value\", randomWordGenerator())\n",
    "data.write.format(\"delta\").save(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  2|   to|\n",
      "|  3|   to|\n",
      "|  1|hello|\n",
      "|  6|happy|\n",
      "|  7|   to|\n",
      "|  4|   to|\n",
      "|  5|   am|\n",
      "|  0|    I|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(files[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Streaming write ######\n"
     ]
    }
   ],
   "source": [
    "# Stream writes to the table\n",
    "\n",
    "print(\"####### Streaming write ######\")\n",
    "streamingDf = spark.readStream.format(\"delta\").load(files[0])\n",
    "stream = streamingDf \\\n",
    "    .selectExpr(\"value as word\") \\\n",
    "    .groupBy(\"word\")\\\n",
    "    .count() \\\n",
    "    .writeStream\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoint\")\\\n",
    "    .start(files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "| word|count|\n",
      "+-----+-----+\n",
      "|hello|    1|\n",
      "|happy|    1|\n",
      "|   am|    1|\n",
      "|   to|    4|\n",
      "|    I|    1|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(files[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
