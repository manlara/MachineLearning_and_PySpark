{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Additional Python Libraries\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding environment variable `PYSPARK_SUBMIT_ARGS`\n",
      "--packages com.databricks:spark-csv_2.11:1.5.0,org.postgresql:postgresql:42.2.9,org.mongodb.spark:mongo-spark-connector_2.11:2.4.1 pyspark-shell\n"
     ]
    }
   ],
   "source": [
    "# Need postgres\n",
    "# https://mvnrepository.com/artifact/org.postgresql/postgresql\n",
    "from spark_libs import spark_submit\n",
    "packages = [\"com.databricks:spark-csv_2.11:1.5.0\", \n",
    "            \"org.postgresql:postgresql:42.2.9\",\n",
    "            \"org.mongodb.spark:mongo-spark-connector_2.11:2.4.1\"]\n",
    "spark_submit(packages=packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkFiles\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "from spark_connections import postgresUrlProperties, createMongoURI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get or create Spark session\n",
    "\n",
    "app_name = \"spark-mongo-postgres-save-parquet\"\n",
    "spark = SparkSession.builder.appName(app_name).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mongodb://host.docker.internal:27017/users.user_data\n",
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- active_user: boolean (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- street_address: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mongo_connection = {\n",
    "    \"hostname\": \"host.docker.internal\",\n",
    "    \"port\": \"27017\"\n",
    "}\n",
    "mongo_database = \"users\"\n",
    "sample_size = 1000 # how many rows to use to determine the schema\n",
    "\n",
    "collection = \"user_data\"\n",
    "mongoURI = createMongoURI(mongo_connection, mongo_database, collection)\n",
    "print(mongoURI)\n",
    "\n",
    "df_user_data = spark.read \\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"uri\",mongoURI) \\\n",
    "    .option(\"sampleSize\", sample_size) \\\n",
    "    .load()\n",
    "df_user_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data as Parquet\n",
    "\n",
    "Parquet, an open source file format for Hadoop. Parquet stores nested data structures in a flat columnar format. Compared to a traditional approach where data is stored in row-oriented approach, parquet is more efficient in terms of storage and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_mode = \"overwrite\" # options: error, append, overwrite\n",
    "\n",
    "df_user_data.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .options() \\\n",
    "    .mode(save_mode) \\\n",
    "    .save(\"user_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- active_user: boolean (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- street_address: string (nullable = true)\n",
      " |-- username: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"parquet\").load(\"user_data.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
